{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeLyLhDOXNj-",
        "outputId": "6a17c92f-078f-4773-ec98-4fd4587240d5",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA8n2wwtx7NL",
        "outputId": "3de97317-7db6-4826-a74e-d075e54c7c08",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from TextBlob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "pip install TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcH6PKz2fK-j",
        "outputId": "c38f2769-352f-4cf0-b29a-0c11baab045c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "qJRA64yDEvmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sMQQfaLnztwC"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MDrTnDZWKil3"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set GPU Device"
      ],
      "metadata": {
        "id": "eLPly2OFE7ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITX-KLpIOeAx",
        "outputId": "287000f4-aa1b-4dfd-a6bb-c500c9927d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# use the GPU\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Texts and Labels\n"
      ],
      "metadata": {
        "id": "ZhZjjvEeFDik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U7G6TmpLsTcL"
      },
      "outputs": [],
      "source": [
        "class TextLoader:\n",
        "    \"\"\"Download the files and load them into a dataframe.\n",
        "\n",
        "    This TextLoader performs file loading through tags filtering and concatting\n",
        "    texts by positive and negative tags.\n",
        "    \n",
        "    Attributes:\n",
        "    ----------\n",
        "    pos_label: Pre-defined positive tag\n",
        "    neg_label: Pre-defined negative tag\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, data_name, pos_label, neg_label):\n",
        "        self.data_name = data_name\n",
        "        self.pos_label = pos_label\n",
        "        self.neg_label = neg_label\n",
        "\n",
        "    # import the dataset (txt file) line by line\n",
        "    def load_text(self):\n",
        "        raw_data = []\n",
        "        for item in [\"_text.txt\", \"_labels.txt\"]:\n",
        "            with open(self.data_name + item, 'rb') as f:\n",
        "                texts = []\n",
        "                for line in f:\n",
        "                    texts.append(line.decode(errors='ignore').lower().strip())\n",
        "            raw_data.append(texts)\n",
        "        return raw_data\n",
        "\n",
        "    def sample_text(self):\n",
        "        raw_data = self.load_text()\n",
        "        raw_texts = raw_data[0]\n",
        "        raw_labels = raw_data[1]\n",
        "\n",
        "        pos_indexs = [i for i, x in enumerate(raw_labels) if x == self.pos_label]\n",
        "        neg_indexs = [i for i, x in enumerate(raw_labels) if x == self.neg_label]\n",
        "        pos_raw_text = [raw_texts[i] for i in pos_indexs]\n",
        "        neg_raw_text = [raw_texts[i] for i in neg_indexs]\n",
        "\n",
        "        # concat negative and positive texts \n",
        "        texts = pos_raw_text + neg_raw_text\n",
        "\n",
        "        # we know the order in texts variable, so we can label it accordingly\n",
        "        labels = np.array([1]*len(pos_raw_text) + [0]*len(neg_raw_text))\n",
        "\n",
        "        return texts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "yQdvlUOhHR16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tcd8AEGmc1z-"
      },
      "outputs": [],
      "source": [
        "class TweetCleaner:\n",
        "    \"\"\"Clean tweet text data.\n",
        "\n",
        "    This Cleaner performes text pre-processing through a series of \n",
        "    operations while creating a NLP application.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: series of length n_tweets\n",
        "    \"\"\"\n",
        "     \n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.ps = PorterStemmer()\n",
        "\n",
        "    def preprocess(self):\n",
        "        corpus = []\n",
        "\n",
        "        for text in self.text:\n",
        "            text = text.replace('\\\\n', ' ')     # Filtering Line breaks\n",
        "            text = re.sub('@\\S+', '', text)     # Filtering User names\n",
        "            text = re.sub('#', '', text)        # Change Tags into texts\n",
        "            new_text = [self.ps.stem(word) for word in text.split()]    # Stemming\n",
        "            text = ' '.join(new_text)           \n",
        "            text = emoji.demojize(text)         # turn emoji into text\n",
        "            text = re.sub(':', ' ', text)       # Change Tags into texts\n",
        "            text = re.sub('[^a-z _]+', '', text) # Filtering Symbols and nums\n",
        "            corpus.append(text)\n",
        "        \n",
        "        print(\"Data preprocessing done.\")\n",
        "        return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "J5YK6GozHcKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QwJ6AmF85NPM"
      },
      "outputs": [],
      "source": [
        "class ContextGenerator:\n",
        "    \"\"\"Generate the context\n",
        "    \"\"\"  \n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        max_len = 0\n",
        "        tokenized_texts = []\n",
        "        word2idx = {}\n",
        "\n",
        "        # Add <pad> and <unk> tokens to the vocabulary\n",
        "        word2idx['<pad>'] = 0\n",
        "        word2idx['<unk>'] = 1\n",
        "\n",
        "        # Building our vocab from the corpus starting from index 0\n",
        "        idx = 2\n",
        "        for sent in texts:\n",
        "            tokenized_sent = nlp(sent)\n",
        "            # Add `tokenized_sent` to `tokenized_texts`\n",
        "            tokenized_texts.append(tokenized_sent)\n",
        "            # Add new token to `word2idx`\n",
        "            for token in tokenized_sent:\n",
        "                # string any token objects are different things, be careful.\n",
        "                if token.text not in word2idx:\n",
        "                    word2idx[token.text] = idx\n",
        "                    idx += 1\n",
        "\n",
        "                # Update `max_len`\n",
        "            max_len = max(max_len, len(tokenized_sent))\n",
        "            \n",
        "        return tokenized_texts, word2idx, max_len\n",
        "\n",
        "    def encode(self, tokenized_texts, word2idx, max_len):\n",
        "        input_ids = []\n",
        "        for tokenized_sent in tokenized_texts:\n",
        "            # Pad sentences to max_len\n",
        "            tokenized_padded_sent = list(tokenized_sent) + ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "    \n",
        "            # Encode tokens to input_ids\n",
        "            input_id = [word2idx.get(str(token)) for token in tokenized_padded_sent]\n",
        "            input_ids.append(input_id)\n",
        "        \n",
        "        return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "TovvP3D-Hrqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tEwijs_-2SqO"
      },
      "outputs": [],
      "source": [
        "class Loader:\n",
        "    \"\"\"Create DataLoader in Torch.\n",
        "\n",
        "    This Loader encapsulates TweetCleaner, ContextGenerator and generate\n",
        "    Torch DataLoader.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    dl: object of TweetCleaner\n",
        "    \"\"\"\n",
        "    def __init__(self, dl):\n",
        "        self.dl = dl\n",
        "    \n",
        "    def loader(self):\n",
        "        texts, labels = self.dl.sample_text()\n",
        "        cleaner = TweetCleaner(texts)\n",
        "        texts = cleaner.preprocess()\n",
        "\n",
        "        cg = ContextGenerator()\n",
        "        tokenized_texts, word2idx, max_len = cg.tokenize(texts)\n",
        "        input_ids = cg.encode(tokenized_texts, word2idx, max_len)\n",
        "\n",
        "        # Convert data type to torch.Tensor\n",
        "        inputs = torch.from_numpy(input_ids)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        # Specify batch_size\n",
        "        batch_size = 2\n",
        "\n",
        "        # Create DataLoader for training data\n",
        "        data = TensorDataset(inputs, labels)\n",
        "        sampler = RandomSampler(data)\n",
        "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "        return dataloader, len(word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Model Class"
      ],
      "metadata": {
        "id": "68be5UFAIkSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aT7ceUgQ9UII"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=0,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        The constructor for CNN class.\n",
        "        Args:\n",
        "            vocab_size (int): Vocabulary size.\n",
        "            embed_dim (int): Dimension of word vectors.\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100]\n",
        "            n_classes (int): Number of classes. Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        # Random Embedding layer\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "        \n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "        \n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Implement for dataset 1 (Joy-Anger)"
      ],
      "metadata": {
        "id": "GYGgRESeIsY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zquabjXYI9ra",
        "outputId": "ef927335-5ced-48a5-85c7-755a6da2a8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing done.\n",
            "Data preprocessing done.\n",
            "Data preprocessing done.\n"
          ]
        }
      ],
      "source": [
        "train_dl = TextLoader(\"train\", \"1\", \"0\")\n",
        "test_dl = TextLoader(\"test\", \"1\", \"0\")\n",
        "val_dl = TextLoader(\"val\", \"1\", \"0\")\n",
        "\n",
        "train_DL = Loader(train_dl)\n",
        "test_DL = Loader(test_dl)\n",
        "val_DL = Loader(val_dl)\n",
        "\n",
        "trainloader, size = train_DL.loader()\n",
        "testloader, item1 = test_DL.loader()\n",
        "valloader, item2= val_DL.loader()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader Implement for dataset 2 (Joy-Sadness)"
      ],
      "metadata": {
        "id": "Xvx7NqL4IzPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl_2 = TextLoader(\"train\", \"1\", \"3\")\n",
        "test_dl_2 = TextLoader(\"test\", \"1\", \"3\")\n",
        "val_dl_2 = TextLoader(\"val\", \"1\", \"3\")\n",
        "\n",
        "train_DL_2 = Loader(train_dl_2)\n",
        "test_DL_2 = Loader(test_dl_2)\n",
        "val_DL_2 = Loader(val_dl_2)\n",
        "\n",
        "trainloader_2, size_2 = train_DL_2.loader()\n",
        "testloader_2, item1 = test_DL_2.loader()\n",
        "valloader_2, item2= val_DL_2.loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OKDerGUtBcU",
        "outputId": "45c17811-0c19-42a4-a39c-c8f2cbb6079d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing done.\n",
            "Data preprocessing done.\n",
            "Data preprocessing done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement CNN Model for both datasets"
      ],
      "metadata": {
        "id": "hBYnAJjAJAOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-PT4b5TR9ct_"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate CNN model\n",
        "model = CNN(embed_dim=300,\n",
        "            vocab_size=size,\n",
        "            filter_sizes=[3, 5, 7],\n",
        "            num_filters=[100, 100, 100],\n",
        "            num_classes=2,\n",
        "            dropout=0.2)\n",
        "    \n",
        "# Send model to `device` (GPU/CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Instantiate Adadelta optimizer\n",
        "optimizer = optim.Adadelta(model.parameters(),\n",
        "                               lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Instantiate CNN model\n",
        "model_2 = CNN(embed_dim=300,\n",
        "            vocab_size=size_2,\n",
        "            filter_sizes=[3, 5, 7],\n",
        "            num_filters=[100, 100, 100],\n",
        "            num_classes=2,\n",
        "            dropout=0.5)\n",
        "    \n",
        "# Send model to `device` (GPU/CPU)\n",
        "model_2.to(device)\n",
        "\n",
        "# Instantiate Adadelta optimizer\n",
        "optimizer = optim.Adam(model_2.parameters(),\n",
        "                               lr=0.01)"
      ],
      "metadata": {
        "id": "OIo-8R-KtaBx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training Function"
      ],
      "metadata": {
        "id": "4axEZx_EJNcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BVZfty0--l_D"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def training(model):\n",
        "    # Specify loss function\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    avg_val_losses = []\n",
        "    for epoch_i in range(30):\n",
        "        total_loss = 0\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "        for step, batch in enumerate(trainloader_2):\n",
        "        \n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate the average loss over the entire training data\n",
        "            avg_train_loss = total_loss / len(trainloader_2) \n",
        "        print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on dataset 1 (Joy-Anger)"
      ],
      "metadata": {
        "id": "6WtX9oMfJTCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = training(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGuzvXEC-J2Z",
        "outputId": "add94f9f-911d-4eb4-ea30-a9beb20c9259"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss \n",
            "------------------------------------------------------------\n",
            "   1    |   0.653515  \n",
            "   2    |   0.638520  \n",
            "   3    |   0.627388  \n",
            "   4    |   0.619170  \n",
            "   5    |   0.607263  \n",
            "   6    |   0.598289  \n",
            "   7    |   0.596140  \n",
            "   8    |   0.582525  \n",
            "   9    |   0.572793  \n",
            "  10    |   0.556810  \n",
            "  11    |   0.553192  \n",
            "  12    |   0.546828  \n",
            "  13    |   0.531434  \n",
            "  14    |   0.520958  \n",
            "  15    |   0.512214  \n",
            "  16    |   0.499813  \n",
            "  17    |   0.490283  \n",
            "  18    |   0.479410  \n",
            "  19    |   0.469557  \n",
            "  20    |   0.453643  \n",
            "  21    |   0.441909  \n",
            "  22    |   0.427926  \n",
            "  23    |   0.413588  \n",
            "  24    |   0.406225  \n",
            "  25    |   0.389115  \n",
            "  26    |   0.385705  \n",
            "  27    |   0.369956  \n",
            "  28    |   0.362443  \n",
            "  29    |   0.349529  \n",
            "  30    |   0.339331  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on dataset 2 (Joy-Sadness)"
      ],
      "metadata": {
        "id": "WMieF77xJisb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = training(model_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s86BHCLWuE6p",
        "outputId": "ccadadd6-f1f7-4d61-b206-2e0908b762a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss \n",
            "------------------------------------------------------------\n",
            "   1    |   9.792317  \n",
            "   2    |   7.531464  \n",
            "   3    |   3.841768  \n",
            "   4    |   2.760264  \n",
            "   5    |   3.595385  \n",
            "   6    |   4.363451  \n",
            "   7    |   4.173997  \n",
            "   8    |   3.812296  \n",
            "   9    |   4.160670  \n",
            "  10    |   3.702138  \n",
            "  11    |   7.077987  \n",
            "  12    |   4.935472  \n",
            "  13    |   4.310239  \n",
            "  14    |   2.552106  \n",
            "  15    |   3.575094  \n",
            "  16    |   2.767444  \n",
            "  17    |   2.406714  \n",
            "  18    |   2.269345  \n",
            "  19    |   3.105965  \n",
            "  20    |   7.041555  \n",
            "  21    |   3.585576  \n",
            "  22    |   3.568698  \n",
            "  23    |   5.641461  \n",
            "  24    |   2.841337  \n",
            "  25    |   2.979980  \n",
            "  26    |   3.597410  \n",
            "  27    |   3.010501  \n",
            "  28    |   1.742867  \n",
            "  29    |   4.502463  \n",
            "  30    |   3.319356  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ai416AeefdH",
        "outputId": "d465e965-fdfe-47dd-f840-e50eaee0b597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Function"
      ],
      "metadata": {
        "id": "tcgdK55jJnHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b_Y6iCuCsAr1"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import BinaryF1Score\n",
        "from torchmetrics import Accuracy\n",
        "classes = ('joy','sadness')\n",
        "\n",
        "def evaluate(loader, model):\n",
        "    with torch.no_grad():\n",
        "        # prepare to count predictions for each class\n",
        "        correct_pred = {classname: 0 for classname in classes}\n",
        "        total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "        targets = torch.empty(0, dtype=torch.float64)\n",
        "        outputs = torch.empty(0, dtype=torch.float64)\n",
        "        preds = torch.empty(0, dtype=torch.float64)\n",
        "        outputs = outputs.cuda(device)\n",
        "        targets = targets.cuda(device)\n",
        "        preds = preds.cuda(device)\n",
        "\n",
        "        for inputs, label in enumerate(loader):\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in label)\n",
        "            output = model(b_input_ids)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            \n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(b_labels, predicted):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "            \n",
        "            outputs = torch.cat((outputs, output))\n",
        "            targets = torch.cat((targets, b_labels))\n",
        "            preds = torch.cat((preds, predicted))\n",
        "        \n",
        "        accuracy = 0\n",
        "        for classname, correct_count in correct_pred.items():\n",
        "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "            print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "\n",
        "    f1 = BinaryF1Score().to(device)\n",
        "    f1_score = f1(preds, targets)\n",
        "    \n",
        "\n",
        "    print(f'F1_macro of the network: {f1_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on Model 1\n",
        "accuracy and F1-macro on the training and development set"
      ],
      "metadata": {
        "id": "34MJA2iTJsmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joy-Anger Training Dataset"
      ],
      "metadata": {
        "id": "RLUdiQcbJ957"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aGGlL8-dbub",
        "outputId": "314ffd79-655a-405e-8425-c8c4d8db3f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: joy   is 98.7 %\n",
            "Accuracy for class: anger is 80.9 %\n",
            "F1_macro of the network: 0.8821138143539429\n"
          ]
        }
      ],
      "source": [
        "evaluate(trainloader, model.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joy-Anger Validating Dataset"
      ],
      "metadata": {
        "id": "Jwq_IitCKHAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-joyI6dYa7GA",
        "outputId": "b18eaaad-95f8-4895-dd5f-70d5eaf1b5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: joy   is 63.1 %\n",
            "Accuracy for class: anger is 28.9 %\n",
            "F1_macro of the network: 0.30434781312942505\n"
          ]
        }
      ],
      "source": [
        "evaluate(valloader, model.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on Model 2\n",
        "In this part, we tuned 14 groups of parameters and found the best 3 choices. The output only shows one of them because the codes are not encapsulated perfectly. We wrote OOP-oriented classes most of the time, but there are still some repetitive codes. Optimizing the project codes is time-consuming, however. In this situation, we tuned the parameters manually and wrote the results into a table in the lab report."
      ],
      "metadata": {
        "id": "9EYXM5RTKL8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joy-Anger Testing Dataset\n",
        "accuracy and F1-macro on the testing set with model 2."
      ],
      "metadata": {
        "id": "UEJ8p_KSKSkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(testloader, model_2.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19PvkZpTuOSa",
        "outputId": "03518ddc-373f-4b41-a399-974f37d42e10"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: joy   is 20.4 %\n",
            "Accuracy for class: anger is 87.2 %\n",
            "F1_macro of the network: 0.5601436495780945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joy-Sadness Testing Dataset\n",
        "accuracy and F1-macro on the testing set with model 2."
      ],
      "metadata": {
        "id": "OQSoTIbaKW6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrleoeD3a26T",
        "outputId": "785ae1df-74ad-4e54-8e1c-e0f2369cad1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: joy   is 9.7 %\n",
            "Accuracy for class: sadness is 86.3 %\n",
            "F1_macro of the network: 0.6106719374656677\n"
          ]
        }
      ],
      "source": [
        "evaluate(testloader_2, model_2.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xvckT6XAmRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}